{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michael-borck/isys2001-worksheets/blob/main/text_sumarizier_extended.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmVEXRTA1wZn"
      },
      "source": [
        "# Text Summarisation\n",
        "\n",
        "One of the themes has been to summarise online content and produce a report throughout the semester. Our solution is to convert everything to text, summarise the text, and use the summary in the final report. This notebook is another step in the process. The lessons from this notebook are:\n",
        "* understanding how to **reference** online tutorials, videos etc.\n",
        "* extending and reusing working code. In this case, a text summariser.\n",
        "* build on and use advanced concepts without implementing them, in this case, machine learning.\n",
        "\n",
        "> This notebook contains a lot of exploration. If you want to see the final answer, jump to the bottom of the notebook.\n",
        "\n",
        "# Build a summariser\n",
        "\n",
        "This section is based on the YouTube video [AI Text Summarization with Hugging Face Transformers in 4 Lines of Python](https://youtu.be/TsfLm5iiYb4)\n",
        "\n",
        "As Information Systems professionals, we use our skills to be aware of advanced concepts and think about how you can meet the organisational Using *Hugging Face Transformers*, you can leverage a pre-trained summarisation pipeline to start summarising content. In this section, we will: \n",
        "1. Installing Hugging Face Transformers\n",
        "2. Building a summarisation pipeline\n",
        "3. Run model/pipeline to summarisation\n",
        "4. **Investigate way to reuse the pipeline**\n",
        "\n",
        "> [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) free state-of-the-art pre-trained machine learning models for processing text, images, audio and video. See the project website for more information.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Dependencies\n",
        "!pip install transformers -q"
      ],
      "metadata": {
        "id": "CT-MEcHCfhDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_gjhmg416Jw"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "from transformers import pipeline\n",
        "\n",
        "# load sumarisation pipeline \n",
        "summary_pipeline = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "\n",
        "# Let us copy-n-paste some text\n",
        "article = '''\n",
        "A lack of transparency and reporting standards in the scientifc community has led to increasing and widespread\n",
        "concerns relating to reproduction and integrity of results. As an omics science, which generates vast amounts of data and\n",
        "relies heavily on data science for deriving biological meaning, metabolomics is highly vulnerable to irreproducibility. The\n",
        "metabolomics community has made substantial eforts to align with FAIR data standards by promoting open data formats,\n",
        "data repositories, online spectral libraries, and metabolite databases. Open data analysis platforms also exist; however,\n",
        "they tend to be infexible and rely on the user to adequately report their methods and results. To enable FAIR data science\n",
        "in metabolomics, methods and results need to be transparently disseminated in a manner that is rapid, reusable, and fully\n",
        "integrated with the published work. To ensure broad use within the community such a framework also needs to be inclusive\n",
        "and intuitive for both computational novices and experts alike\n",
        "'''\n",
        "\n",
        "# Run the summariser pipeline\n",
        "summary = summary_pipeline(article, max_length = 50, min_length= 20)\n",
        "\n",
        "# What does a summary look like?\n",
        "print(summary)\n",
        "\n",
        "# By inspection of output, 'summary' is a list.  The first element of the list is a dictionary.\n",
        "# The key to the dictionary is 'summary_text'.\n",
        "\n",
        "# Extract and display the summarised text \n",
        "text = summary[0]['summary_text'] # get first element, then extract the value for key 'summary text\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay that seemed to work.  How can we reuse this code?  How about we create a function that take an 'article' and returns a summary"
      ],
      "metadata": {
        "id": "EJUEJE3DfvhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def summarise(article):\n",
        "  summary_pipeline = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "  summary = summary_pipeline(article, max_length = 50, min_length= 20)\n",
        "  text = summary[0]['summary_text'] # get first element, then extract the value for key 'summary text\n",
        "  return text\n"
      ],
      "metadata": {
        "id": "LQajbB93giuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A quick test."
      ],
      "metadata": {
        "id": "6pBTSJ6Wg731"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "some_text = '''\n",
        "A lack of transparency and reporting standards in the scientifc community has led to increasing and widespread\n",
        "concerns relating to reproduction\n",
        "'''\n",
        "\n",
        "print(summarise(some_text))"
      ],
      "metadata": {
        "id": "qB3Tck54g-5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Umm... it worked, but with a warning on max_length.   We could reduce the max length or add a check that we have at least 50 words.  Our reasoning (design decision) is that it doesn't really make sense to sumarise say one sentance. We could pick any minimun size, but 50 seems like a good number.\n",
        "\n",
        "But first, how do I count words in a string?  We did something like this in an earlier notebook where we counted the spaces.  We could search the internat for some code snippets.  We can use the the string method `split()`."
      ],
      "metadata": {
        "id": "mrEgaztkh48F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(str.split)"
      ],
      "metadata": {
        "id": "7pyqFWiQjIpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So `split()` returns a list of words.  The `len()` of the list will be the word count.  Let us try it.\n"
      ],
      "metadata": {
        "id": "YbVR_fCEjWSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "some_text = '''\n",
        "A lack of transparency and reporting standards in the scientifc community has led to increasing and widespread\n",
        "concerns relating to reproduction\n",
        "'''\n",
        "\n",
        "count = len(some_text.split())\n",
        "print(count)"
      ],
      "metadata": {
        "id": "T3BcaeAkjyVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us update the function to include this check.  We will also add a doc string.  I choosen to use an `assert` statement, but you could do something similar with an `if` statement."
      ],
      "metadata": {
        "id": "gyPYdf_MkryF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def summarise(article):\n",
        "  '''Returns a summary of a text.  The length of the text has to be greater than 50 words'''\n",
        "  assert len(article.split()) > 50, 'Please make sure your text has at least 50 words'\n",
        "\n",
        "  summary_pipeline = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "  summary = summary_pipeline(article, max_length = 50, min_length= 20)\n",
        "  text = summary[0]['summary_text'] # get first element, then extract the value for key 'summary text\n",
        "  return text"
      ],
      "metadata": {
        "id": "ac9uWyG-ksjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "some_text = '''\n",
        "A lack of transparency and reporting standards in the scientifc community has led to increasing and widespread\n",
        "concerns relating to reproduction\n",
        "'''\n",
        "\n",
        "print(summarise(some_text))"
      ],
      "metadata": {
        "id": "sxHTLUMWltqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great the assertion worked."
      ],
      "metadata": {
        "id": "VKI-tSypl_Vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigger_text='''\n",
        "A lack of transparency and reporting standards in the scientifc community has led to increasing and widespread\n",
        "concerns relating to reproduction and integrity of results. As an omics science, which generates vast amounts of data and\n",
        "relies heavily on data science for deriving biological meaning, metabolomics is highly vulnerable to irreproducibility. The\n",
        "metabolomics community has made substantial eforts to align with FAIR data standards by promoting open data formats,\n",
        "data repositories, online spectral libraries, and metabolite databases.\n",
        "'''\n",
        "\n",
        "print(summarise(bigger_text))"
      ],
      "metadata": {
        "id": "Koub-Hmel-68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNBfzaztdGsh"
      },
      "source": [
        "Okay that is working well. Let us start to use our hard work\n",
        "\n",
        "How about we summarise each page of a PDF.\n",
        "\n",
        "    Get/Download the PDF\n",
        "    for each page in the PDF\n",
        "        extract the text\n",
        "        summarise the text\n",
        "\n",
        "Google search find a simple tutorial [How to Extract Text From PDF File In Python - PyMuPDF](https://youtu.be/RQTiyQzowLQ)\n",
        "\n",
        "# Summarise PDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup requirements\n",
        "\n",
        "# Get a PDF\n",
        "# Google Scholar 'Toward collaborative open data science in metabolomics using Jupyter Notebooks and cloud computing'\n",
        "!wget https://link.springer.com/content/pdf/10.1007/s11306-019-1588-0.pdf\n",
        "\n",
        "# Install required packages\n",
        "!pip install PyMuPDF -q"
      ],
      "metadata": {
        "id": "VG6EaaMjndO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note: There is maximum sequence length for the default model.  As we are only demonstrating the concept we will truncate text size to 400 words**"
      ],
      "metadata": {
        "id": "9H37MqezpZVX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_A434v6AdI9I"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import fitz\n",
        "\n",
        "# Use PDF downloaded\n",
        "pdf = \"s11306-019-1588-0.pdf\"\n",
        "doc = fitz.open(pdf)\n",
        "for page in doc:\n",
        "  article = page.get_text(\"Text\")\n",
        "  # The model has a limit size, first 400 words on eachpage\n",
        "  # Implement a better solution\n",
        "  article = ' '.join(article.split()[:400])\n",
        "  # Run the summariser pipeline\n",
        "  text = summarise(article)\n",
        "  print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets make it a function."
      ],
      "metadata": {
        "id": "UldQua0O3Qz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sumarise_pdf(pdf):\n",
        "  ''' Sumarise the first 400 words on each page of the PDF'''\n",
        "  import fitz\n",
        "  doc = fitz.open(pdf)\n",
        "  for page in doc:\n",
        "    article = page.get_text(\"Text\")\n",
        "    # The model has a limit size, first 400 words on eachpage\n",
        "  # Implement a better solution\n",
        "    article = ' '.join(article.split()[:400])\n",
        "    # Run the summariser pipeline\n",
        "    text = summarise(article)\n",
        "    return text"
      ],
      "metadata": {
        "id": "CM1fsGQP3P_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What would you have to do to make this sumarise a folder with many PDFs?\n",
        "\n",
        "1. Get the list of files in the directory\n",
        "2. for each file in the list, call `summarise_pdf()`\n",
        "\n",
        "How would you save the summaries? What information should you save?  Probably need to know the source document, page number and the summary of the page.  Maybe a Python dictionary or SQL database?\n",
        "\n",
        "# Scrape text from Webpage\n",
        "\n",
        "Lets use the pipeline to summarise a web page.  Another google search and after looking\n",
        "at a few articles, YouTube videos I settled on this page: [2 Ways to Extract Text From HTML Using Python](https://computersciencehub.io/python/extract-text-from-html-using-python/)\n"
      ],
      "metadata": {
        "id": "le0G_lOv6icA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "\n",
        "# get webpage\n",
        "req = Request(\"https://en.wikipedia.org/wiki/Python_(programming_language)\")\n",
        "html_page = urlopen(req)\n",
        "\n",
        "soup = BeautifulSoup(html_page, features=\"html.parser\")\n",
        "\n",
        "# remove all 'script' and 'style' elements\n",
        "for script in soup([\"script\", \"style\"]):\n",
        "    script.extract()    # rip it out\n",
        "\n",
        "# get text\n",
        "text = soup.get_text()\n",
        "\n",
        "# break into lines and remove leading and trailing space on each\n",
        "lines =  text.splitlines()\n",
        "\n",
        "# remove empty lines\n",
        "lines = [x for x in lines if x]\n",
        "\n",
        "# combine into one body of text\n",
        "text = ' '.join(lines)\n",
        "# split into words\n",
        "text = text.split()\n",
        "# get first 400 words\n",
        "text = text[:400]\n",
        "# join words into text\n",
        "text = ' '.join(text)\n",
        "\n",
        "summarise(text)"
      ],
      "metadata": {
        "id": "vSusobUmm5Tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets make it a function!\n",
        "\n"
      ],
      "metadata": {
        "id": "YJvSrYKYsesV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "\n",
        "def summarise_webpage(URL):\n",
        "  ''' Sumarise the first 400 words on a website'''\n",
        "  # get webpage\n",
        "  req = Request(URL)\n",
        "  html_page = urlopen(req)\n",
        "  soup = BeautifulSoup(html_page, features=\"html.parser\")\n",
        "\n",
        "  # remove all 'script' and 'style' elements\n",
        "  for script in soup([\"script\", \"style\"]):\n",
        "      script.extract()    # rip it out\n",
        "\n",
        "  text = soup.get_text() # get text\n",
        "  lines =  text.splitlines() # break into lines\n",
        "  lines = [x for x in lines if x] # remove empty lines\n",
        "  text = ' '.join(lines) # combine into one body of text\n",
        "  text = text.split() # split into words\n",
        "  text = text[:400] # get first 400 words\n",
        "  text = ' '.join(text) # join words into text\n",
        "\n",
        "  return summarise(text)\n",
        "\n",
        "text = summarise_webpage(\"https://en.wikipedia.org/wiki/Python_(programming_language)\")\n",
        "print(text)"
      ],
      "metadata": {
        "id": "PFMntyUDs3dI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What would you have to do to make this sumarise a many URLs?\n",
        "\n",
        "1. Create a list of URLs\n",
        "2. for each URL in the list, call `summarise_web_page()`\n",
        "\n",
        "How would you save the summaries? What information should you save?  Probably need to know the source document, page number and the summary of the page.  Maybe a Python dictionary or SQL database?"
      ],
      "metadata": {
        "id": "2WJOWb9wuCYB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Final Solution"
      ],
      "metadata": {
        "id": "mUNCdJR_26Hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Dependencies\n",
        "!pip install transformers -q\n",
        "!pip install PyMuPDF -q\n",
        "\n",
        "# Get a PDF\n",
        "# Google Scholar 'Toward collaborative open data science in metabolomics using Jupyter Notebooks and cloud computing'\n",
        "!wget https://link.springer.com/content/pdf/10.1007/s11306-019-1588-0.pdf"
      ],
      "metadata": {
        "id": "kOqByVidQSOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "from transformers import pipeline\n",
        "import fitz\n",
        "\n",
        "\n",
        "def summarise(article):\n",
        "  '''Returns a summary of a text.  The length of the text has to be greater than 50 words'''\n",
        "  assert len(article.split()) > 50, 'Please make sure your text has at least 50 words'\n",
        "\n",
        "  summary_pipeline = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "  summary = summary_pipeline(article, max_length = 50, min_length= 20)\n",
        "  text = summary[0]['summary_text'] # get first element, then extract the value for key 'summary text\n",
        "  return text\n",
        "\n",
        "\n",
        "def sumarise_pdf(pdf):\n",
        "  ''' Sumarise the first 400 words on each page of the PDF'''\n",
        "\n",
        "  doc = fitz.open(pdf)\n",
        "  for page in doc:\n",
        "    article = page.get_text(\"Text\")\n",
        "    # The model has a limit size, \n",
        "    # first 400 words on eachpage\n",
        "    article = ' '.join(article.split()[:400])\n",
        "    # Run the summariser pipeline\n",
        "    text = summarise(article)\n",
        "    return text\n",
        "\n",
        "\n",
        "def summarise_webpage(URL):\n",
        "  ''' Sumarise the first 400 words on a website'''\n",
        "  # get webpage\n",
        "  req = Request(URL)\n",
        "  html_page = urlopen(req)\n",
        "  soup = BeautifulSoup(html_page, features=\"html.parser\")\n",
        "\n",
        "  # remove all 'script' and 'style' elements\n",
        "  for script in soup([\"script\", \"style\"]):\n",
        "      script.extract()    # rip it out\n",
        "\n",
        "  text = soup.get_text() # get text\n",
        "  lines =  text.splitlines() # break into lines\n",
        "  lines = [x for x in lines if x] # remove empty lines\n",
        "  text = ' '.join(lines) # combine into one body of text\n",
        "  text = text.split() # split into words\n",
        "  text = text[:400] # get first 400 words\n",
        "  text = ' '.join(text) # join words into text\n",
        "\n",
        "  return summarise(text)\n",
        "\n",
        "# Main Program \n",
        "\n",
        "print(\"PDF Summary\")\n",
        "print(sumarise_pdf(\"s11306-019-1588-0.pdf\"))\n",
        "\n",
        "print(\"Webiste Summary\")\n",
        "print(summarise_webpage(\"https://en.wikipedia.org/wiki/Python_(programming_language)\"))"
      ],
      "metadata": {
        "id": "B_qn9Qq-3oqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dXRG7elxoPEO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "text_sumarizier_extended.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}